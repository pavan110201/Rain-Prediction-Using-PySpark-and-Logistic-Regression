# -*- coding: utf-8 -*-
"""Py Spark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GSh1lCCI28szGZEk4JNCBIZeQ9HQTTLK
"""

# Downloading and extracting Spark 3.5.1

!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz
!tar xf spark-3.5.1-bin-hadoop3.tgz
!pip install -q findspark

# Setting up Spark environment variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.1-bin-hadoop3"

# Mounting to Google Drive
from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd
import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.types import *

# creating a spark session
spark = SparkSession.builder.appName("weatherAUS").getOrCreate()

#Libraries

import os
import sys
import tempfile
import shutil
import pyspark.sql.functions as F
from pyspark.sql import SparkSession
from pyspark.mllib.stat import Statistics
from pyspark.mllib.util import MLUtils
from pyspark.sql.types import *
from pyspark.sql.functions import col, expr,isnan,when,count,year, month, dayofmonth
from pyspark.sql.types import StringType, FloatType, IntegerType, DoubleType
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler,Imputer, StandardScaler, StringIndexer
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.sql.functions import lit

#Reading the dataset
data = spark.read.csv('/content/drive/MyDrive/weatherAUS.csv',inferSchema=True,header=True)
data.show(5)

#Printing the data schema
data.printSchema()

"""1) Exploring categorical variable names"""

#converting sttrings to float
df_stf = data.select(*(col(c).cast(FloatType()) if c in ["MinTemp","MaxTemp","Rainfall","Evaporation","WindGustSpeed","WindSpeed9am","Sunshine","WindSpeed3pm", "Humidity9am", "Humidity3pm", "Pressure9am", "Pressure3pm", "Cloud9am", "Cloud3pm", "Temp9am", "Temp3pm", "RISK_MM"
] else col(c) for c in data.columns))
df_stf.printSchema()

#printing categorical variables in the dataset
schema = df_stf.schema
string_columns = [field.name for field in schema if isinstance(field.dataType, StringType)]
print("Columns containing string values:", string_columns)

"""2) categorical variables having null values"""

# Identifying categorical variables with null values
df_cv = df_stf.select([count(when(col(c).contains('None') | \
                            col(c).contains('NULL') | \
                            col(c).contains('NA') | \
                            (col(c) == '' ) | \
                            col(c).isNull() | \
                            isnan(c), c
                           )).alias(c)
                    for c in string_columns])
print('Displaying categorical variables with null value counts')
df_cv.show()

categorical_columns_with_null = [c for c in string_columns if df_cv.select(c).first()[c] > 0]
print(f'Number of categorical columns with null values are: {categorical_columns_with_null},count: {len(categorical_columns_with_null)}')

"""3) Frequency count of each categorical variable"""

#Printing Frequency count of categorical variables
for column in string_columns:
    frequency_count = df_cv.groupBy(column).count()
    print(f"\nFrequency count for '{column}':")
    frequency_count.show(100,truncate=False)
    distinct_values_count = df_cv.select(column).distinct().count()
    print(f"Number of datapoints in each distinct value for {column}: {distinct_values_count}")
    print()

"""4) First five rows of the dataset"""

#printing first five rows of the data
df_stf.show(5)

"""5) Avilable columns of the dataset"""

#printing avilable columns in the dataset
df_stf.columns

"""6) Droping Risk_MM column"""

#Dropping risk column
df_dr = df_stf.drop("RISK_MM")

"""7) Summary of the dataset"""

#summary of the data
df_dr.summary().show()

"""8) First five rows of categrical Variables"""

#printing first five rows of categrical Variables
df_dr.select(string_columns).show(5, truncate=False)

"""9) Decomposing date field into year, month, and day fields

"""

#Decomposing date field
df = df_dr.withColumn("Year", year(col("Date"))) \
         .withColumn("Month", month(col("Date"))) \
         .withColumn("Day", dayofmonth(col("Date")))
df_dd = df.drop("Date")
df_dd.show(5)

"""10) Number of unique Locations in dataset"""

#printing number of unique locations in the dataset
df_dd.select("Location").distinct().count()

"""11) Number of times each location appears in the dataset"""

# no of times each location appears in the dataset
Unique_locations_appears = df_dd.groupBy("Location").count()
Unique_locations_appears.show(Unique_locations_appears.count(), truncate=False)

"""14) Replacing null values of dataset with median value for numerical columns"""

#Replacing null values of data with median for numericals
schema = df_dd.schema
numerical_columns = [field.name for field in schema if isinstance(field.dataType, FloatType)]
medians = {}
for column in numerical_columns:
    median_value = df_dd.approxQuantile(column, [0.5], 0.01)[0]
    medians[column] = median_value
for column in numerical_columns:
    median_value = medians[column]
    df_mn = df_mn.withColumn(column, when(col(column).isNull(), median_value).otherwise(col(column)))
df_mn.show()

""" Replacing null values of data wuth mode in the categrical columns"""

#Replacing null values of data with mode for categrical
string_columns = [field.name for field in schema if isinstance(field.dataType, StringType)]
for column in string_columns:
    mode_value = df_mn.groupBy(column).count().orderBy("count", ascending=False).first()[0]
    df_mc = df_mn.fillna({column: mode_value})
df_mc.show()

"""12) performing one hot encoding"""

# OneHotEncoding for string columns
indexers = [StringIndexer(inputCol=column, outputCol=column + "_index", handleInvalid="keep") for column in string_columns]
encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol= indexer.getInputCol() + "_vec") for indexer in indexers]

"""13) RainTomorrow is the label, and all other fields are features"""

# Using vec_Assembler to assemble features into a single vector
vec_assembler = VectorAssembler(inputCols=[
    'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',
    'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'Year', 'Month', 'Day',
    'Location_vec', 'WindGustDir_vec', 'WindDir9am_vec', 'WindDir3pm_vec', 'RainToday_vec'
], outputCol='features')
label_indexer = StringIndexer(inputCol="RainTomorrow", outputCol="RainTomorrowLabel")

"""15) Normalization of data"""

from pyspark.ml.feature import MinMaxScaler
from pyspark.ml.linalg import Vectors
from pyspark.sql import SparkSession

# Initialize MinMaxScaler
scaler = MinMaxScaler(inputCol="features", outputCol="normalized_features")

"""16) Training with Logistic Regression model on the training dataset (70% 30% split)."""

from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
lr_model = LogisticRegression(featuresCol="normalized_features", labelCol="RainTomorrowLabel")

#Creating pipeline
pipeline = Pipeline(stages=indexers + encoders + [vec_assembler, scaler, label_indexer, lr_model])
pipeline.getStages()

#spliting the dataset
train_data, test_data = df_mc.randomSplit([0.7,.3], seed=64)

#printing no of rows in trainig and testing for the dataset
print("Number of rows in training set:", train_data.count())
print("Number of rows in testing set:", test_data.count())
fit_model = pipeline.fit(train_data)
results = fit_model.transform(test_data)

#printing the results of the model
results.show(100)

"""17) Predicting the RainTomorrow for the test set"""

#performing testing
my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',
                                       labelCol='RainTomorrowLabel')
results.select('RainTomorrowLabel','prediction').show(100)

"""18) confusion matrix and TP, TN,
FP,FN and  accuracy and F1 score of your model.
"""

from pyspark.mllib.evaluation import MulticlassMetrics

# Calculate and display the confusion matrix
confusion_matrix = MulticlassMetrics(results.select('prediction', 'RainTomorrowLabel').rdd.map(lambda row: (row.prediction, row.RainTomorrowLabel))).confusionMatrix().toArray()
print("Confusion Matrix:")
print(confusion_matrix)

# Calculate key metrics and print
TP, TN, FP, FN = confusion_matrix[0][0], confusion_matrix[1][1], confusion_matrix[0][1], confusion_matrix[1][0]
print("TP:", TP, "\nTN:", TN, "\nFP:", FP, "\nFN:", FN)
accuracy, F1_score = (TP + TN) / (TP + TN + FP + FN), 2 * TP / (2 * TP + FP + FN)
print("Accuracy:", accuracy, "\nF1 Score:", F1_score)