
# Downloading and extracting Spark 3.5.1
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz
!tar xf spark-3.5.1-bin-hadoop3.tgz
!pip install -q findspark


# Setting up Spark environment variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.1-bin-hadoop3"



# Mounting to Google Drive
from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd
import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.types import *

# creating a spark session
spark = SparkSession.builder.appName("weatherAUS").getOrCreate()


import os
import sys
import tempfile
import shutil
import pyspark.sql.functions as F
from pyspark.sql import SparkSession
from pyspark.mllib.stat import Statistics
from pyspark.mllib.util import MLUtils
from pyspark.sql.types import *
from pyspark.sql.functions import col, expr,isnan,when,count,year, month, dayofmonth
from pyspark.sql.types import StringType, FloatType, IntegerType, DoubleType
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler,Imputer, StandardScaler, StringIndexer
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.sql.functions import lit
     


#Reading the dataset
data = spark.read.csv('/content/drive/MyDrive/weatherAUS.csv',inferSchema=True,header=True)
data.show(5)



#Printing the data schema
data.printSchema()


